<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <meta name="description"
        content="ReFORM">
  <meta name="keywords" content="Offline reinforcement learning, Support constraint, Flow model">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <title>ReFORM</title>

  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro"
        rel="stylesheet">

  <link rel="stylesheet" href="./static/css/bulma.min.css">
  <link rel="stylesheet" href="./static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="./static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="./static/css/fontawesome.all.min.css">
  <link rel="stylesheet"
        href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="./static/css/index.css">
  <!-- <link rel="icon" href="./static/images/favicon.png"> -->
  <link rel="icon" type="image/svg+xml" href="./static/images/favicon.svg" />
  <link rel="icon" type="image/png" href="./static/images/favicon.png" />

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script defer src="./static/js/fontawesome.all.min.js"></script>
  <!-- <script src="./static/js/bulma-carousel.min.js"></script>
  <script src="./static/js/bulma-slider.min.js"></script>
  <script src="./static/js/index.js"></script> -->
  <script src='https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.4/MathJax.js?config=default'></script>
</head>
<body>

<nav class="navbar" role="navigation" aria-label="main navigation">
  <div class="navbar-brand">
    <a role="button" class="navbar-burger" aria-label="menu" aria-expanded="false">
      <span aria-hidden="true"></span>
      <span aria-hidden="true"></span>
      <span aria-hidden="true"></span>
    </a>
  </div>
  <div class="navbar-menu">
    <div class="navbar-start" style="flex-grow: 1; justify-content: center;">
      <a class="navbar-item" href="https://syzhang092218-source.github.io/">
      <span class="icon">
          <i class="fas fa-home"></i>
      </span>
      </a>

      <div class="navbar-item has-dropdown is-hoverable">
        <a class="navbar-link">
          More Research
        </a>
        <div class="navbar-dropdown">
          <a class="navbar-item" href="https://realm.mit.edu">
            REALM Website
          </a>
        </div>
      </div>
    </div>
  </div>
</nav>


<section class="hero">
  <div class="hero-body">
    <div class="container is-max-desktop">
      <div class="columns is-centered">
        <div class="column has-text-centered">
          <h1 class="title is-1 publication-title">ReFORM: Reflected Flows for On-support Offline RL via Noise Manipulation</h1>
          <div class="is-size-5 publication-authors">
            <span class="author-block">
              <a href="https://syzhang092218-source.github.io/">Songyuan Zhang</a><sup>1</sup>,</span>
            <span class="author-block">
              <a href="https://oswinso.xyz/">Oswin So</a><sup>1</sup>,</span>
            <span class="author-block">
              <a href="https://sabbirahmad26.github.io/">H. M. Sabbir Ahmad</a><sup>2</sup>,</span>
            <span class="author-block">
                <a href="https://ericyangyu.github.io/">Eric Yang Yu</a><sup>1</sup>,</span>
            <span class="author-block">
              <a href="https://www.linkedin.com/in/matthew-cleaveland-4775abba/">Matthew Cleaveland</a><sup>3</sup>,</span>
            <span class="author-block">
              <a href="https://www.blackmitchell.com/">Mitchell Black</a><sup>3</sup>,</span>
            <span class="author-block">
              <a href="http://chuchu.mit.edu/">Chuchu Fan</a><sup>1</sup></span>
          </div>

          <div class="is-size-5 publication-authors">
            <span class="author-block"><sup>1</sup>Massachusetts Institute of Technology &nbsp&nbsp&nbsp&nbsp<sup>2</sup>Boston University &nbsp&nbsp&nbsp&nbsp<sup>3</sup>MIT Lincoln Laboratory</span>
          </div>

          <div class="column has-text-centered">
            <div class="publication-links">
              <!-- PDF Link. -->
              <span class="link-block">
                <a href="https://openreview.net/forum?id=YvFsyRReeN"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fas fa-file-pdf"></i>
                  </span>
                  <span>Paper</span>
                </a>
              </span>
              <!-- <span class="link-block">
                <a href="https://arxiv.org/abs/2504.15425"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="ai ai-arxiv"></i>
                  </span>
                  <span>arXiv</span>
                </a>
              </span> -->
              <!-- Video Link. -->
              <!-- <span class="link-block">
                <a href="https://youtu.be/TODO"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fab fa-youtube"></i>
                  </span>
                  <span>Supplementary Video</span>
                </a>
              </span> -->
              <!-- <span class="link-block">
                <a href="https://www.youtube.com/watch?v=aCcbrz8S83k"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fab fa-youtube"></i>
                  </span>
                  <span>Talk</span>
                </a>
              </span> -->
              <!-- Code Link. -->
              <span class="link-block">
                <a href="https://github.com/MIT-REALM/reform"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fab fa-github"></i>
                  </span>
                  <span>Code</span>
                  </a>
              </span>
              <!-- <span class="link-block">
                <a href="https://roboticsconference.org/program/awards/"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fa fa-trophy"></i>
                  </span>
                  <span>RSS Best Student Paper Award</span>
                  </a>
              </span> -->
            </div>
          </div>
          </div>
        </div>
      </div>
    </div>
  </div>
</section>

<section class="hero teaser">
  <div class="container is-max-desktop">
    <div class="hero-body">
      <h2 class="title is-3"><center><tt>ReFORM</tt>: How to maintain support constraints in offline RL <i>without</i> any statistical distance regularization.</center></h2>
      <div class="hero-body">
        <center>
          <video width="24%" autoplay="" muted="" loop="" controls>
            <source src="videos/antmaze-large.mp4" alt="antmaze-large" type="video/mp4">
          </video>
          <video width="24%" autoplay="" muted="" loop="" controls>
            <source src="videos/cube-single.mp4" alt="cube-single" type="video/mp4">
          </video>
          <video width="24%" autoplay="" muted="" loop="" controls>
            <source src="videos/cube-double.mp4" alt="cube-double" type="video/mp4">
          </video>
          <video width="24%" autoplay="" muted="" loop="" controls>
            <source src="videos/scene.mp4" alt="scene" type="video/mp4">
          </video>
        </center>
      </div>
    </div>
  </div>
</section>

<section class="hero teaser">
  <div class="container is-max-desktop">
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Abstract</h2>
        <div class="content has-text-justified">
          <p>
            Offline reinforcement learning (RL) aims to learn the optimal policy from a fixed
            behavior policy dataset without additional environment interactions. One common 
            challenge that arises in this setting is the out-of-distribution (OOD) error,
            which occurs when the policy leaves the training distribution. Prior methods penalize
             a statistical distance term to keep the policy close to the behavior policy, but
            this constrains policy improvement and may not completely prevent OOD actions.
            Another challenge is that the optimal policy distribution can be multimodal and
            difficult to represent. Recent works apply diffusion or flow policies to address this
            problem, but it is unclear how to avoid OOD errors while retaining policy expressiveness. 
            We propose <tt>ReFORM</tt>, an offline RL method based on flow policies that
            enforces the less restrictive <i>support constraint</i> by construction. <tt>ReFORM</tt> learns a
            behavior cloning (BC) flow policy with a bounded source distribution to capture the support of the
            action distribution, then optimizes a reflected flow that generates bounded noise
            for the BC flow while keeping the support, to maximize the performance. Across
            40 challenging tasks from the OGBench benchmark with datasets of varying quality 
            and using a <i>constant</i> set of hyperparameters for all tasks, <tt>ReFORM</tt> dominates
            all baselines with <i>hand-tuned</i> hyperparameters on the performance profile curves.
          </p>
        </div>
      </div>
    </div>
    <!--/ Abstract. -->
  </div>
</section>


<section class="section">
  
  <div class="container is-max-desktop">
    <!-- Main point 1 -->
    <!-- <h2 class="title is-3">Problem setting</h2>
    <div class="column">
      <div class="columns is-centered">
        <img src="figs/problem_setting.jpg"
          class="center-image"
          style="padding: 0px 50px 0px 0px; max-width: 50%;"
          alt="Problem setting"/>
          <p>
            We consider the multi-agent safe optimal control problem (MASOCP) with <b>discrete-time</b>, <b>unknown</b> dynamics, <b>partial observability</b>, <b>input constraints</b>. Given \(N\) agents, we aim at design distributed policies \(\mu_1, \dots, \mu_N\), such that:
            <br><br>
            The task is done: \(\min_{\pi_1,\dots,\pi_N} \sum_{k=0}^\infty l({x}^k, \pi({x}^k))\),<br>
            following the unknown dynamics: \({x}^{k+1} = f({x}^k, {\pi}({x}^k))\), <br>
            and the agents are safe: \(h_i(o_i^k)\leq 0, \quad o_i^k=O_i({x}^k)\)
          </p>
      </div>
    </div> -->

    <!-- <div class="column"></div>
    <h2 class="title is-3">Epigraph Form</h2>
    <div class="column">
      <div class="content">
        <p>
          As Lagrangian methods usually suffer from unstable training, we use the epigraph form for constrained optimization to improve training stability. First, we deinfe the cost-value function \(V^l\) using the standard optimal control formulation:
          \[
            V^l(x^\tau; \pi) = \sum_{k\geq\tau} l(x^k, \pi(x^k)).
          \]
          We also define the constraint-value function \(V^{h}\) as the maximum constraint violation:
          \[
            V^{h}(x^\tau; \pi) = \max_{k\geq\tau}h(x^k) = \max_{k\geq\tau}\max_i h_i(o_i^k) = \max_i\max_{k\geq\tau} h_i(o_i^k) = \max_i V^{h}_i(o_i^\tau; \pi).
          \]
          Then, we can rewrite the MASOCP as:
          \[
            \min_{\pi_1,\dots,\pi_N} V^l(x^0; \pi)
            \quad \text{s.t. } V^{h}(x^0; \pi) \leq 0.
          \]
          The epigraph form then takes the form:
          \[
            \min_z \; z
            \quad \text{s.t. } \min_{\pi_1,\dots,\pi_N} V(x^0, z; \pi) \leq 0,
          \]
          where \(V(x^0, z; \pi) = \max\left\{\max_i V_i^h(o_i^\tau;\pi),V^l(x^\tau;\pi)-z\right\}\).
        </p>
      </div>
    </div> -->

    <!-- <div class="column"></div>
    <h2 class="title is-3">Distributed Epigraph Form</h2>
    <div class="column">
      <div class="content">
        <p>
          The epigraph form can be solved in a distributed fashion by each agent. First, we define the total value function for each agent as
          \[
            V_i(x^\tau, z; \pi) = \max\left\{V_i^h(o_i^\tau;\pi),V^l(x^\tau;\pi)-z\right\}.
          \]
          Then, we can rewrite the epigraph form as:
          \[
            \min_{z} \; z
            \quad \text{s.t. } \min_{\pi_1,\dots,\pi_N} \max_i V_i(x^0, z; \pi) \leq 0.
          \]
          This decomposes the original problem into an unconstrained inner problem over policy \(\pi\) and a constrained outer problem over \(z\). During offline training, we solve the inner problem: for parameter \(z\), find the optimal policy \(\pi(\cdot,z)\) to minimize \(V(x^0,z;\pi)\).
          Note that the optimal policy of the inner problem depends on \(z\).
          During execution, we solve the outer problem online to get the minimal \(z\) that satisfies constraints. Using this \(z\) in the \(z\)-conditioned policy \(\pi(\cdot,z)\) found in the inner problem gives us the optimal policy for the overall epigraph form MASOCP (EF-MASOCP).
        </p>
      </div> 
    </div>-->

    <!-- <div class="column"></div>
    <h2 class="title is-3">Solving the outer problem during distributed execution</h2>
    <div class="column">
      <div class="content">
        <p>
          First, we provide the following theorem as the fundation of the distributed execution:
          <div  style="border-radius: 5px; background-color:rgb(238, 238, 238);">
            <p>
              <i>Theorem 1:</i> Assume no two unique values of \(z\) achieves the same unique cost. Then, the outer problem of EF-MASOCP is equivalent to the following:
              \[
                \begin{aligned}
                  z &= \max_i \; z_i \\
                  z_i &= \min_{z'} \; z' \quad \text{s.t. } V^h_i(x^0, z'; \pi) \leq 0.
                \end{aligned}
              \]
            </p>
          </div>
          This enables computing \(z\) <i>without</i> the use of the centralized \(V^l\) during execution. Specifically, each agent \(i\) solves the local problem for \(z_i\), which is a 1-dimensional optimization problem and can be efficiently solved using root-finding methods, then communicates \(z_i\) among the other agents to obtain the maximum. Furthermore, we observe experimentally that the agents can achieve low cost while maintaining safety even if \(z_i\) is not communicated. Thus, we do not include \(z_i\) communication for our method.
        </p>
      </div>
    </div> -->

    <div class="column"></div>
    <h2 class="title is-3"><tt>ReFORM</tt>: Overall framework</h2>
    <div class="column">
      <div class="content">
        <img src="figs/reform.png"
          class="center-image"
          width="90%"
          alt="algorithm structure"/>
      </div>
    </div>

    <!-- <div class="column"></div>
    <h2 class="title is-3">Simulation Environments</h2>
    <div class="column">
      <div class="content">
        <img src="figs/MPETarget.jpg"
          width="16.2%"
          alt="MPETarget"/>
          <img src="figs/MPESpread.jpg"
          width="16.2%"
          alt="MPESpread"/>
          <img src="figs/MPEFormation.jpg"
          width="16.2%"
          alt="MPEFormation"/>
          <img src="figs/MPELine.jpg"
          width="16.2%"
          alt="MPELine"/> 
          <img src="figs/MPECorridor.jpg"
          width="16.2%"
          alt="MPECorridor"/>
          <img src="figs/MPEConnectSpread.jpg"
          width="16.2%"
          alt="MPEConnectSpread"/>
          <img src="figs/env_legend.jpg"
          width="100%"
          alt="env_legend"/>
          <p align="center">
            MPE environments.
          </p>
      </div>
    </div>
    <div class="column">
      <div class="content">
        <center>
        <img src="figs/multicheetah.png"
          width="49%"
          alt="multicheetah"/>
        <img src="figs/coupledcheetah.png"
          width="49%"
          alt="coupledcheetah"/>
        </center>
        <p align="center">
          MuJoCo environments, where contact dynamics are included.
        </p>
      </div>
    </div> -->

    <!-- <div class="column"></div>
    <h2 class="title is-3">Numerical Results</h2>
    <div class="column">
      <div class="content">
        <img src="figs/3_safe_cost.jpg"
          class="center-image"
          width="80%"
          alt="main results"/>
      </div>
      <p align="center">
        <b>Comparison on \(N=3\) agents.</b> <tt>Def-MARL</tt> has the best performance by being closest to the top left corner. 
      </p>
    </div>
    <div class="column">
      <div class="content">
        <img src="figs/lagr_train_main.jpg"
          class="center-image"
          width="80%"
          alt="training stability"/>
      </div>
      <p align="center">
        <b>Training stability.</b> <tt>Def-MARL</tt> yields smoother training curves compared to the baselines.
      </p>
    </div>
    <div class="column">
      <div class="content">
        <img src="figs/scalability.jpg"
          class="center-image"
          width="80%"
          alt="large scale training"/>
      </div>
      <p align="center">
        <b>Results on larger-scale MPE.</b> Unlike other methods, <tt>Def-MARL</tt> maintains the best performance.
      </p>
    </div> -->

  <!-- <div class="column"></div>
  <h2 class="title is-3">Related Work</h2>
  <div class="column">
    <div class="content">
      <div class="content has-text-justified">
        <p>
          This work is part of our line of work on designing safe and intelligent control policies for multi-agent systems. Other works on this line includes:
          <br>
          <br>
          <a href="https://mit-realm.github.io/dgppo/">DGPPO</a>: How to extend CBFs elegantly for safe MARL.
          <br>
          <a href="https://mit-realm.github.io/gcbf-website/">GCBFv0</a>: Generalizable distributed safe controllers for 1000+ agents.
          <br>
          <a href="https://mit-realm.github.io/gcbfplus/">GCBF+</a>: Generalizable distributed safe controllers for 1000+ agents, an improved version of GCBFv0.
          <br>
          <br>
          For a survey of the field of learning safe control for multi-robot systems, see <a rel="survey" href="https://arxiv.org/pdf/2311.13714.pdf">this paper</a>.
        </p>
      </div>
    </div> -->
  </div>
</section>


<section class="section" id="BibTeX">
  <div class="container is-max-desktop content">
    <h2 class="title">BibTeX</h2>
    <pre><code>@inproceedings{zhang2026reform,
      title={Re{FORM}: Reflected Flows for On-support Offline {RL} via Noise Manipulation},
      author={Zhang, Songyuan and So, Oswin and Ahmad, H M Sabbir and Yu, Eric Yang and Cleaveland, Matthew and Black, Mitchell and Fan, Chuchu},
      booktitle={The Fourteenth International Conference on Learning Representations},
      year={2026},
}</code></pre>
  </div>
</section>


<footer class="footer">
  <div class="container">
    <div class="content has-text-centered">
    </div>
    <div class="columns is-centered">
      <div class="column is-8">
        <div class="content">
          <p>
            This website is licensed under a <a rel="license"
                                                href="http://creativecommons.org/licenses/by-sa/4.0/">Creative
            Commons Attribution-ShareAlike 4.0 International License</a>.
            This webpage template is based that used by <a href="https://github.com/nerfies/nerfies.github.io">Nerfies</a>. 
            We sincerely thank <a href="https://keunhong.com/">Keunhong Park</a> for developing and open-sourcing this template.
          </p>
        </div>
      </div>
          </p>
        </div>
      </div>
    </div>
  </div>
</footer>


</body>
</html>
