<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <meta name="description"
        content="ReFORM">
  <meta name="keywords" content="Offline reinforcement learning, Support constraint, Flow model">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <title>ReFORM</title>

  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro"
        rel="stylesheet">

  <link rel="stylesheet" href="./static/css/bulma.min.css">
  <link rel="stylesheet" href="./static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="./static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="./static/css/fontawesome.all.min.css">
  <link rel="stylesheet"
        href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="./static/css/fonts.css">
  <link rel="stylesheet" href="./static/css/index.css">
  <!-- <link rel="icon" href="./static/images/favicon.png"> -->
  <link rel="icon" type="image/svg+xml" href="./static/images/favicon.svg" />
  <link rel="icon" type="image/png" href="./static/images/favicon.png" />

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script defer src="./static/js/fontawesome.all.min.js"></script>
  <!-- <script src="./static/js/bulma-carousel.min.js"></script>
  <script src="./static/js/bulma-slider.min.js"></script>
  <script src="./static/js/index.js"></script> -->
  <script src='https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.4/MathJax.js?config=default'></script>
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fontsource/latin-modern-mono@5.0.0/index.css">
</head>
<body>

<nav class="navbar" role="navigation" aria-label="main navigation">
  <div class="navbar-brand">
    <a role="button" class="navbar-burger" aria-label="menu" aria-expanded="false">
      <span aria-hidden="true"></span>
      <span aria-hidden="true"></span>
      <span aria-hidden="true"></span>
    </a>
  </div>
  <div class="navbar-menu">
    <div class="navbar-start" style="flex-grow: 1; justify-content: center;">
      <a class="navbar-item" href="https://syzhang092218-source.github.io/">
      <span class="icon">
          <i class="fas fa-home"></i>
      </span>
      </a>

      <div class="navbar-item has-dropdown is-hoverable">
        <a class="navbar-link">
          More Research
        </a>
        <div class="navbar-dropdown">
          <a class="navbar-item" href="https://realm.mit.edu">
            REALM Website
          </a>
        </div>
      </div>
    </div>
  </div>
</nav>


<section class="hero">
  <div class="hero-body">
    <div class="container is-max-desktop">
      <div class="columns is-centered">
        <div class="column has-text-centered">
          <h1 class="title is-1 publication-title">ReFORM: Reflected Flows for On-support Offline RL via Noise Manipulation</h1>
          <div class="is-size-5 publication-authors">
            <span class="author-block">
              <a href="https://syzhang092218-source.github.io/">Songyuan Zhang</a><sup>1</sup>,</span>
            <span class="author-block">
              <a href="https://oswinso.xyz/">Oswin So</a><sup>1</sup>,</span>
            <span class="author-block">
              <a href="https://sabbirahmad26.github.io/">H. M. Sabbir Ahmad</a><sup>2</sup>,</span>
            <span class="author-block">
                <a href="https://ericyangyu.github.io/">Eric Yang Yu</a><sup>1</sup>,</span><br>
            <span class="author-block">
              <a href="https://www.linkedin.com/in/matthew-cleaveland-4775abba/">Matthew Cleaveland</a><sup>3</sup>,</span>
            <span class="author-block">
              <a href="https://www.blackmitchell.com/">Mitchell Black</a><sup>3</sup>,</span>
            <span class="author-block">
              <a href="http://chuchu.mit.edu/">Chuchu Fan</a><sup>1</sup></span>
          </div>

          <div class="is-size-5 publication-authors">
            <span class="author-block"><sup>1</sup>Massachusetts Institute of Technology &nbsp&nbsp&nbsp&nbsp<sup>2</sup>Boston University &nbsp&nbsp&nbsp&nbsp<sup>3</sup>MIT Lincoln Laboratory</span>
          </div>

          <div class="column has-text-centered">
            <div class="publication-links">
              <!-- PDF Link. -->
              <span class="link-block">
                <a href="https://openreview.net/forum?id=YvFsyRReeN"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fas fa-file-pdf"></i>
                  </span>
                  <span>Paper</span>
                </a>
              </span>
              <span class="link-block">
                <a href="https://github.com/MIT-REALM/reform"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fab fa-github"></i>
                  </span>
                  <span>Code</span>
                  </a>
              </span>
            </div>
          </div>
        </div>
      </div>
    </div>
  </div>
</section>

<section class="hero teaser">
  <div class="container is-max-desktop">
    <div class="hero-body">
      <h2 class="title is-3"><center><tt>ReFORM</tt>: How to maintain support constraints in offline RL <i>without</i> any statistical distance regularization.</center></h2>
      <h2 class="title is-3"><center>You no longer need to tune regularization weights!</center></h2>
      <div class="hero-body">
        <center>
          <video width="24%" autoplay="" muted="" loop="" controls>
            <source src="media/antmaze-large.mp4" alt="antmaze-large" type="video/mp4">
          </video>
          <video width="24%" autoplay="" muted="" loop="" controls>
            <source src="media/cube-single.mp4" alt="cube-single" type="video/mp4">
          </video>
          <video width="24%" autoplay="" muted="" loop="" controls>
            <source src="media/cube-double.mp4" alt="cube-double" type="video/mp4">
          </video>
          <video width="24%" autoplay="" muted="" loop="" controls>
            <source src="media/scene.mp4" alt="scene" type="video/mp4">
          </video>
        </center>
      </div>
    </div>
  </div>
</section>

<section class="section">
  <div class="container is-max-desktop">
    <div class="column">
      <div class="content">
        <h2 class="title is-3">Overview</h2>
        <ul>
          <li>
            <b><tt>ReFORM</tt></b> is an offline RL algorithm which utilizes flow-based policies to <b>enforce the support constraint by construction</b>, avoiding out-of-distribution errors <i>without constraining policy improvement</i>.
          </li>
          <li>
            We propose applying <b>reflected flow</b> to generate constrained multimodal noise for the BC flow policy, thereby mitigating OOD errors while <b>maintaining the multimodal policy</b>. 
          </li>
          <li>
            Extensive experiments on 40 challenging tasks with datasets of different qualities demonstrate that, with a <b>constant set of hyperparameters</b>, <tt>ReFORM</tt> dominates all baselines using flow policy structures with the <i>best hand-tuned hyperparameters</i> on the performance profile curve.
          </li>
        </ul>        
      </div>
    </div>

    <div class="column">
      <div class="content">
        <h2 class="title is-3">Challenges</h2>
        <ul>
          <li>
            <b>Out of Distribution (OOD) problem</b> is a common challenge in offline RL, where the learned policy may generate actions outside the support of the behavior policy, leading to erroneous value estimates and poor performance.
          </li>
          <li>
            Prior works attempt to address the OOD issue by keeping the learned policy close to the behavior policy (dataset) by <b>regularizing a statistical distance</b> to the behavior policy. However, this approach <i>constrains policy improvement</i> and may <i>not</i> fully prevent OOD actions. Also they often require <i>careful hyperparameter tuning</i> for different tasks and datasets.
          </li>
          <li>
            While avoiding OOD errors, it is also important to maintain the <b>expressiveness of the policy</b>.
          </li>
        </ul>        
      </div>
    </div>

    <div class="column">
      <div class="content">
        <h2 class="title is-3">Method</h2>
        <img src="media/reform.png"
            class="center-image"
            width="90%"
            alt="algorithm structure"/>
        <ul>
          <li>
            <tt>ReFORM</tt> starts by <b>learning a BC flow policy</b> (<span style="color: #AEAEAE">gray</span> arrows), which transforms a simple <i>bounded</i> source distribution \(q_\mathrm{BC}=\mathcal{U}(\mathcal B_l^d)\) (uniform distribution in \(d\)-dimensional hypersphere with radius \(l\)) to a target distribution \(p_\mathrm{BC}\) that matches the dataset \(\mathcal D\). The BC policy captures the support of the behavior policy due to the bounded source distribution, which allows us to enforce the <i>support constraint</i> in offline RL.
          </li>
          <li>
            At the same time, <tt>ReFORM</tt> learns a <b>reflected flow-based noise generator</b> (<span style="color: #335E96">blue</span> arrows) that generate a <i>manipulated</i> source distribution \(\tilde q_\mathrm{BC}\) for the BC policy, such that the manipulated target distribution \(\tilde p_\mathrm{BC}\) maximizes the \(Q\) value while <i>staying inside the support</i> (<span style="color:  #C94D2E">red</span>) of the BC policy. This allows us to avoid OOD errors while maintaining policy expressiveness.
          </li>
        </ul>
        <center>
          <figure style="width: 13%; display: inline-block; vertical-align: top; margin: 0;">
            <img src="media/toy/BC-Gaussian.png" alt="BC" width="100%"/>
            <figcaption class="video-caption">
              <tt>BC</tt>
            </figcaption>
          </figure>
          <figure style="width: 13%; display: inline-block; vertical-align: top; margin: 0;">
            <img src="media/toy/dsrl.png" alt="DSRL" width="100%"/>
            <figcaption class="video-caption">
              <tt>DSRL</tt>
            </figcaption>
          </figure>
          <figure style="width: 13%; display: inline-block; vertical-align: top; margin: 0;">
            <img src="media/toy/ifql.png" alt="IFQL" width="100%"/>
            <figcaption class="video-caption">
              <tt>IFQL</tt>
            </figcaption>
          </figure>
          <figure style="width: 13%; display: inline-block; vertical-align: top; margin: 0;">
            <img src="media/toy/fql-small.png" alt="FQL(S)" width="100%"/>
            <figcaption class="video-caption">
              <tt>FQL(S)</tt>
            </figcaption>
          </figure>
          <figure style="width: 13%; display: inline-block; vertical-align: top; margin: 0;">
            <img src="media/toy/fql-medium.png" alt="ReFORM" width="100%"/>
            <figcaption class="video-caption">
              <tt>FQL(M)</tt>
            </figcaption>
          </figure>
          <figure style="width: 13%; display: inline-block; vertical-align: top; margin: 0;">
            <img src="media/toy/fql-large.png" alt="FQL(L)" width="100%"/>
            <figcaption class="video-caption">
              <tt>FQL(L)</tt>
            </figcaption>
          </figure>
          <figure style="width: 13%; display: inline-block; vertical-align: top; margin: 0;">
            <img src="media/toy/reform.png" alt="ReFORM" width="100%"/>
            <figcaption class="video-caption">
              <tt>ReFORM</tt>
            </figcaption>
          </figure>
        </center>
        <ul>
          <li>
            Comparing with the baselines, only <tt>ReFORM</tt> is able to generate <b>multimodal actions</b> with <b>maximum Q values</b> while <b>staying within the support</b> (<span style="color:  #C94D2E">red</span>) of the BC policy. 
          </li>
        </ul>
      </div>
    </div>

    <div class="column">
      <div class="content">
        <h2 class="title is-3">Experiments</h2>
        <h3>Tasks</h3>
        <center>
          <figure style="width: 24%; display: inline-block; vertical-align: top; margin: 0;">
            <video width="100%" autoplay muted loop controls>
              <source src="media/antmaze-large.mp4" type="video/mp4">
            </video>
            <figcaption class="video-caption">
              <p class="env-name">antmaze-large</p>
            </figcaption>
          </figure>
          <figure style="width: 24%; display: inline-block; vertical-align: top; margin: 0;">
            <video width="100%" autoplay muted loop controls>
              <source src="media/cube-single.mp4" type="video/mp4">
            </video>
            <figcaption class="video-caption">
              <p class="env-name">cube-single</p>
            </figcaption>
          </figure>
          <figure style="width: 24%; display: inline-block; vertical-align: top; margin: 0;">
            <video width="100%" autoplay muted loop controls>
              <source src="media/cube-double.mp4" type="video/mp4">
            </video>
            <figcaption class="video-caption">
              <p class="env-name">cube-double</p>
            </figcaption>
          </figure>
          <figure style="width: 24%; display: inline-block; vertical-align: top; margin: 0;">
            <video width="100%" autoplay muted loop controls>
              <source src="media/scene.mp4" type="video/mp4">
            </video>
            <figcaption class="video-caption">
              <p class="env-name">scene</p>
            </figcaption>
          </figure>
        </center>
        <ul>
          <li>
            We evaluate <tt>ReFORM</tt> and the baselines on 40 tasks from the <a href="https://seohong.me/projects/ogbench/">OGBench</a> offline RL benchmark designed in four environments, including locomotion tasks and manipulation tasks. 
          </li>
        </ul>
        <center>
          <figure style="width: 24%; display: inline-block; vertical-align: top; margin: 0;">
            <img src="media/maze-navigate.png" alt="clean-dataset-antmaze" width="100%"/>
            <figcaption class="video-caption">
              <span class="dataset-name">clean</span> dataset for <span class="env-name">antmaze</span>
            </figcaption>
          </figure>
          <figure style="width: 24%; display: inline-block; vertical-align: top; margin: 0;">
            <img src="media/maze-explore.png" alt="noisy-dataset-antmaze" width="100%"/>
            <figcaption class="video-caption">
              <span class="dataset-name">noisy</span> dataset for <span class="env-name">antmaze</span>
            </figcaption>
          </figure>
        </center>
        <ul>
          <li>
            We use two kinds of datasets, <span class="dataset-name">clean</span> and <span class="dataset-name">noisy</span>. The <span class="dataset-name">clean</span> dataset consists of random environment trajectories generated by an expert policy. The <span class="dataset-name">noisy</span> dataset consists of random trajectories generated by a highly suboptimal and noisy policy. 
          </li>
          <li>
            We define the <b>normalized score</b> for each task as the return normalized by the minimum and maximum returns across all algorithms.
          </li>
        </ul>      
        <h3>Results</h3>
        <img src="media/results/reward_performance_profile.png"
            class="center-image"
            width="90%"
            alt="performance profile"/>
        <ul>
          <li>
            For a given normalized score \(\tau\) (x-axis), the performance profile shows the probability that a given method achieves a score \(\geq\tau\). On the <span class="dataset-name">clean</span> dataset, <tt>ReFORM</tt> <b>achieves greater scores with higher probabilities than all other baselines</b>. The same is true on the <span class="dataset-name">noisy</span> dataset except for a small set of normalized scores around 0.9 where <tt>ReFORM</tt> and <tt>FQL(S)</tt> have similar probabilities within the statistical margins. Note that <tt>ReFORM</tt> uses <b>a constant set of hyperparameters for all tasks</b>, while all baselines use hand-tuned hyperparameters for each task.
          </li>
        </ul>
        <center>
          <img src="media/results/antmaze-large-navigate_normalized_reward_boxplot.png"
              width="24%"
              alt="antmaze-large-navigate_normalized_reward_boxplot"/>
          <img src="media/results/cube-single-play_normalized_reward_boxplot.png"
              width="24%"
              alt="cube-single-play_normalized_reward_boxplot"/>
          <img src="media/results/cube-double-play_normalized_reward_boxplot.png"
              width="24%"
              alt="cube-double-play_normalized_reward_boxplot"/>
          <img src="media/results/scene-play_normalized_reward_boxplot.png"
              width="24%"
              alt="scene-play_normalized_reward_boxplot"/>
          <img src="media/results/antmaze-large-explore_normalized_reward_boxplot.png"
              width="24%"
              alt="antmaze-large-explore_normalized_reward_boxplot"/>
          <img src="media/results/cube-single-noisy_normalized_reward_boxplot.png"
              width="24%"
              alt="cube-single-noisy_normalized_reward_boxplot"/>
          <img src="media/results/cube-double-noisy_normalized_reward_boxplot.png"
              width="24%"
              alt="cube-double-noisy_normalized_reward_boxplot"/>
          <img src="media/results/scene-noisy_normalized_reward_boxplot.png"
              width="24%"
              alt="scene-noisy_normalized_reward_boxplot"/>
        </center>
        <ul>
          <li>
            We present bar plots of the interquantile means (IQM) of the normalized scores for each algorithm in each environment with the <span class="dataset-name">clean</span> dataset (top row) and the <span class="dataset-name">noisy</span> dataset (bottom row). We can observe that <tt>ReFORM</tt> consistently achieves the best or comparable results in all environments with both datasets, with a constant set of hyperparameters. <tt>DSRL</tt> and <tt>FQL(M)</tt> generally perform the second and third best in environments with the <span class="dataset-name">clean</span> dataset. However, their performance drops when the <span class="dataset-name">noisy</span> dataset is used.
          </li>
        </ul>
      </div>
    </div>
  </div>
</section>

<section class="hero teaser">
  <div class="container is-max-desktop">
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Abstract</h2>
        <div class="content has-text-justified">
          <p>
            Offline reinforcement learning (RL) aims to learn the optimal policy from a fixed
            behavior policy dataset without additional environment interactions. One common 
            challenge that arises in this setting is the out-of-distribution (OOD) error,
            which occurs when the policy leaves the training distribution. Prior methods penalize
             a statistical distance term to keep the policy close to the behavior policy, but
            this constrains policy improvement and may not completely prevent OOD actions.
            Another challenge is that the optimal policy distribution can be multimodal and
            difficult to represent. Recent works apply diffusion or flow policies to address this
            problem, but it is unclear how to avoid OOD errors while retaining policy expressiveness. 
            We propose <tt>ReFORM</tt>, an offline RL method based on flow policies that
            enforces the less restrictive <i>support constraint</i> by construction. <tt>ReFORM</tt> learns a
            behavior cloning (BC) flow policy with a bounded source distribution to capture the support of the
            action distribution, then optimizes a reflected flow that generates bounded noise
            for the BC flow while keeping the support, to maximize the performance. Across
            40 challenging tasks from the OGBench benchmark with datasets of varying quality 
            and using a <i>constant</i> set of hyperparameters for all tasks, <tt>ReFORM</tt> dominates
            all baselines with <i>hand-tuned</i> hyperparameters on the performance profile curves.
          </p>
        </div>
      </div>
    </div>
  </div>
</section>

<section class="section" id="BibTeX">
  <div class="container is-max-desktop content">
    <h2 class="title">BibTeX</h2>
    <pre><code>@inproceedings{zhang2026reform,
      title={Re{FORM}: Reflected Flows for On-support Offline {RL} via Noise Manipulation},
      author={Zhang, Songyuan and So, Oswin and Ahmad, H M Sabbir and Yu, Eric Yang and Cleaveland, Matthew and Black, Mitchell and Fan, Chuchu},
      booktitle={The Fourteenth International Conference on Learning Representations},
      year={2026},
}</code></pre>
  </div>
</section>


<footer class="footer">
  <div class="container">
    <div class="content has-text-centered">
    </div>
    <div class="columns is-centered">
      <div class="column is-8">
        <div class="content">
          <p>
            This website is licensed under a <a rel="license"
                                                href="http://creativecommons.org/licenses/by-sa/4.0/">Creative
            Commons Attribution-ShareAlike 4.0 International License</a>.
            This webpage template is based that used by <a href="https://github.com/nerfies/nerfies.github.io">Nerfies</a>. 
            We sincerely thank <a href="https://keunhong.com/">Keunhong Park</a> for developing and open-sourcing this template.
          </p>
        </div>
      </div>
    </div>
  </div>
</footer>


</body>
</html>
